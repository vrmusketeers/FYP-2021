{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BrainGNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1vzYxkzEPEI4-X4iJbZvoEWU4FTTCZSco","authorship_tag":"ABX9TyMJlMwJjNBVB0FpeLoLL0iM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JCeBOf3Mi4Cm"},"source":["## BrainGNN \n","The goal here is to create a graph neural network with ROI-aware graph convolutional (Ra-GConv) layers that leverage the topological and functional information of fMRI. The ROI-selection pooling layers allow for selecting ROIs which are important for predicting, while regularizations such as topK pooling ensure better ROI selection. The eventual classifier is binary in nature"]},{"cell_type":"code","metadata":{"id":"VcEAe46LU1YO"},"source":["!pip install nilearn\n","!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n","!pip install torch-geometric\n","!pip install deepdish\n","!pip install tensorboardX"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMM9UG8kk3Xq"},"source":["## Download custom BrainGNN implementation and the source code "]},{"cell_type":"code","metadata":{"id":"NyNTkbUW92Uy"},"source":["! git clone https://github.com/vrmusketeers/FYP-2021.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQ2EZ8_ioyvZ"},"source":["import sys\n","sys.path.append('/content/FYP-2021/notebooks/graph_models/braingnn')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZRiOZ7io41n"},"source":["## Download Data\n","#### To download and process the data - run the following commands after changing the data path in both the files."]},{"cell_type":"code","metadata":{"id":"xR-w-iP7y1NA"},"source":["! python \"01-fetch_data.py\"\n","! python \"02-process_data.py\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z82FlT6IpHXE"},"source":["## Import Libraries"]},{"cell_type":"code","metadata":{"id":"MXcLAxru1l0i"},"source":["import os\n","import numpy as np\n","import argparse\n","import time\n","import copy\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","from tensorboardX import SummaryWriter\n","\n","from imports.ABIDEDataset import ABIDEDataset\n","from torch_geometric.loader import DataLoader\n","from net.braingnn import Network\n","from imports.utils import train_val_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hf5bhJBCpggL"},"source":["## Set parameters\n","The parameters are set to run the model with Adam Optimizer for 100 Epochs. The model is a GNN built using pytorch-geometric"]},{"cell_type":"code","metadata":{"id":"SU835PhL6mbP"},"source":["torch.manual_seed(123)\n","\n","EPS = 1e-10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uA463hnT6qpy"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--epoch', type=int, default=1, help='starting epoch')\n","parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs of training')\n","parser.add_argument('--batchSize', type=int, default=100, help='size of the batches')\n","parser.add_argument('--dataroot', type=str, default='/content/drive/MyDrive/Colab Notebooks/Project/abide_sta/data/ABIDE_pcp/cpac/filt_noglobal', help='Dataset Root')\n","parser.add_argument('--fold', type=int, default=4, help='training which fold')\n","parser.add_argument('--lr', type = float, default=0.01, help='learning rate')\n","parser.add_argument('--stepsize', type=int, default=20, help='scheduler step size')\n","parser.add_argument('--gamma', type=float, default=0.5, help='scheduler shrinking rate')\n","parser.add_argument('--weightdecay', type=float, default=5e-3, help='regularization')\n","parser.add_argument('--lamb0', type=float, default=1, help='classification loss weight')\n","parser.add_argument('--lamb1', type=float, default=0, help='s1 unit regularization')\n","parser.add_argument('--lamb2', type=float, default=0, help='s2 unit regularization')\n","parser.add_argument('--lamb3', type=float, default=0.1, help='s1 entropy regularization')\n","parser.add_argument('--lamb4', type=float, default=0.1, help='s2 entropy regularization')\n","parser.add_argument('--lamb5', type=float, default=0, help='s1 consistence regularization')\n","parser.add_argument('--layer', type=int, default=2, help='number of GNN layers')\n","parser.add_argument('--ratio', type=float, default=0.5, help='pooling ratio')\n","parser.add_argument('--indim', type=int, default=200, help='feature dim')\n","parser.add_argument('--nroi', type=int, default=200, help='num of ROIs')\n","parser.add_argument('--nclass', type=int, default=2, help='num of classes')\n","parser.add_argument('--load_model', type=bool, default=False)\n","parser.add_argument('--save_model', type=bool, default=True)\n","parser.add_argument('--optim', type=str, default='Adam', help='optimization method: SGD, Adam')\n","parser.add_argument('--save_path', type=str, default='/content/drive/MyDrive/Colab Notebooks/Project/abide_sta/model/', help='path to save model')\n","parser.add_argument('-f')\n","opt = parser.parse_args()\n","\n","if not os.path.exists(opt.save_path):\n","    os.makedirs(opt.save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CCoO6Vq61UA"},"source":["#################### Parameter Initialization #######################\n","path = opt.dataroot\n","name = 'ABIDE'\n","save_model = opt.save_model\n","load_model = opt.load_model\n","opt_method = opt.optim\n","num_epoch = opt.n_epochs\n","fold = opt.fold\n","writer = SummaryWriter(os.path.join('./log',str(fold)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SDejAh6Vp0Hq"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"l2WdIvYQ64BI"},"source":["################## Define Dataloader ##################################\n","dataset = ABIDEDataset(path,name)\n","dataset.data.y = dataset.data.y.squeeze()\n","dataset.data.x[dataset.data.x == float('inf')] = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uZsro8JJwgj"},"source":["dataset = dataset.shuffle()\n","\n","train_dataset = dataset[:600]\n","val_dataset = dataset[600:800]\n","test_dataset = dataset[800:1034]\n","\n","train_loader = DataLoader(train_dataset,batch_size=opt.batchSize, shuffle= True)\n","val_loader = DataLoader(val_dataset, batch_size=opt.batchSize, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=opt.batchSize, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5oVdw0nyp3sv"},"source":["## The model \n","A Graph Convolutional Layer with TopKPooling, Batch Normalization layers and a Linear Output Layer that produces only binary results is deviced."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69PghNy5I6-4","executionInfo":{"status":"ok","timestamp":1637801385797,"user_tz":480,"elapsed":17,"user":{"displayName":"Arun Talkad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi90KExK9fe7Qcd7jFvDxW6c5tPpI_Xw89GqJnDuw=s64","userId":"10224658465974025190"}},"outputId":"aa143e1c-3629-4025-c5a2-898fe70b58ea"},"source":["############### Define Graph Deep Learning Network ##########################\n","model = Network(opt.indim,opt.ratio,opt.nclass).to(device)\n","print(model)\n","\n","if opt_method == 'Adam':\n","    optimizer = torch.optim.Adam(model.parameters(), lr= opt.lr, weight_decay=opt.weightdecay)\n","elif opt_method == 'SGD':\n","    optimizer = torch.optim.SGD(model.parameters(), lr =opt.lr, momentum = 0.9, weight_decay=opt.weightdecay, nesterov = True)\n","\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.stepsize, gamma=opt.gamma)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Network(\n","  (n1): Sequential(\n","    (0): Linear(in_features=200, out_features=8, bias=False)\n","    (1): ReLU()\n","    (2): Linear(in_features=8, out_features=6400, bias=True)\n","  )\n","  (conv1): MyNNConv(200, 32)\n","  (pool1): TopKPooling(32, ratio=0.5, multiplier=1)\n","  (n2): Sequential(\n","    (0): Linear(in_features=200, out_features=8, bias=False)\n","    (1): ReLU()\n","    (2): Linear(in_features=8, out_features=1024, bias=True)\n","  )\n","  (conv2): MyNNConv(32, 32)\n","  (pool2): TopKPooling(32, ratio=0.5, multiplier=1)\n","  (fc1): Linear(in_features=128, out_features=32, bias=True)\n","  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc2): Linear(in_features=32, out_features=512, bias=True)\n","  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc3): Linear(in_features=512, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"FkBFR3puqNhK"},"source":["## Loss Functions"]},{"cell_type":"code","metadata":{"id":"dkekMkvDI8A3"},"source":["############################### Define Other Loss Functions ########################################\n","def topk_loss(s,ratio):\n","    if ratio > 0.5:\n","        ratio = 1-ratio\n","    s = s.sort(dim=1).values\n","    res =  -torch.log(s[:,-int(s.size(1)*ratio):]+EPS).mean() -torch.log(1-s[:,:int(s.size(1)*ratio)]+EPS).mean()\n","    return res\n","\n","def consist_loss(s):\n","    if len(s) == 0:\n","        return 0\n","    s = torch.sigmoid(s)\n","    W = torch.ones(s.shape[0],s.shape[0])\n","    D = torch.eye(s.shape[0])*torch.sum(W,dim=1)\n","    L = D-W\n","    L = L.to(device)\n","    res = torch.trace(torch.transpose(s,0,1) @ L @ s)/(s.shape[0]*s.shape[0])\n","    return res\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDY2VZyII-cP"},"source":["###################### Network Training Function#####################################\n","def train(epoch):\n","    print('train...........')\n","\n","    for param_group in optimizer.param_groups:\n","        print(\"LR\", param_group['lr'])\n","    \n","    model.train()\n","    s1_list = []\n","    s2_list = []\n","    loss_all = 0\n","    step = 0\n","\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        output, w1, w2, s1, s2 = model(data.x, data.edge_index, data.batch, data.edge_attr, data.pos)\n","        s1_list.append(s1.view(-1).detach().cpu().numpy())\n","        s2_list.append(s2.view(-1).detach().cpu().numpy())\n","\n","        loss_c = F.nll_loss(output, data.y)\n","\n","        loss_p1 = (torch.norm(w1, p=2)-1) ** 2\n","        loss_p2 = (torch.norm(w2, p=2)-1) ** 2\n","        loss_tpk1 = topk_loss(s1,opt.ratio)\n","        loss_tpk2 = topk_loss(s2,opt.ratio)\n","        loss_consist = 0\n","        for c in range(opt.nclass):\n","            loss_consist += consist_loss(s1[data.y == c])\n","        \n","        loss = opt.lamb0*loss_c + opt.lamb1 * loss_p1 + opt.lamb2 * loss_p2 \\\n","                   + opt.lamb3 * loss_tpk1 + opt.lamb4 *loss_tpk2 + opt.lamb5* loss_consist\n","        \n","        writer.add_scalar('train/classification_loss', loss_c, epoch*len(train_loader)+step)\n","        writer.add_scalar('train/unit_loss1', loss_p1, epoch*len(train_loader)+step)\n","        writer.add_scalar('train/unit_loss2', loss_p2, epoch*len(train_loader)+step)\n","        writer.add_scalar('train/TopK_loss1', loss_tpk1, epoch*len(train_loader)+step)\n","        writer.add_scalar('train/TopK_loss2', loss_tpk2, epoch*len(train_loader)+step)\n","        writer.add_scalar('train/GCL_loss', loss_consist, epoch*len(train_loader)+step)\n","        step = step + 1\n","\n","        loss.backward()\n","        loss_all += loss.item() * data.num_graphs\n","        optimizer.step()\n","\n","        s1_arr = np.hstack(s1_list)\n","        s2_arr = np.hstack(s2_list)\n","\n","    scheduler.step()\n","\n","    return loss_all / len(train_dataset), s1_arr, s2_arr ,w1,w2\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1yZ7936JOf2"},"source":["###################### Network Testing Function#####################################\n","def test_acc(loader):\n","    model.eval()\n","    correct = 0\n","    for data in loader:\n","        data = data.to(device)\n","        outputs= model(data.x, data.edge_index, data.batch, data.edge_attr,data.pos)\n","        pred = outputs[0].max(dim=1)[1]\n","        correct += pred.eq(data.y).sum().item()\n","\n","    return correct / len(loader.dataset)\n","\n","def test_loss(loader,epoch):\n","    print('testing...........')\n","    model.eval()\n","    loss_all = 0\n","    for data in loader:\n","        data = data.to(device)\n","        output, w1, w2, s1, s2= model(data.x, data.edge_index, data.batch, data.edge_attr,data.pos)\n","        loss_c = F.nll_loss(output, data.y)\n","\n","        loss_p1 = (torch.norm(w1, p=2)-1) ** 2\n","        loss_p2 = (torch.norm(w2, p=2)-1) ** 2\n","        loss_tpk1 = topk_loss(s1,opt.ratio)\n","        loss_tpk2 = topk_loss(s2,opt.ratio)\n","        loss_consist = 0\n","        for c in range(opt.nclass):\n","            loss_consist += consist_loss(s1[data.y == c])\n","        loss = opt.lamb0*loss_c + opt.lamb1 * loss_p1 + opt.lamb2 * loss_p2 \\\n","                   + opt.lamb3 * loss_tpk1 + opt.lamb4 *loss_tpk2 + opt.lamb5* loss_consist\n","\n","        loss_all += loss.item() * data.num_graphs\n","    return loss_all / len(loader.dataset)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJJiCyi2qTQ1"},"source":["## Training\n","The model is trained and the best model stored for future use"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"29VpdH4cJLJa","outputId":"9de3e90b-f7df-4f19-955f-39cb45887fbd"},"source":["#######################################################################################\n","############################   Model Training #########################################\n","#######################################################################################\n","best_model_wts = copy.deepcopy(model.state_dict())\n","best_loss = 1e10\n","for epoch in range(0, num_epoch):\n","    since  = time.time()\n","    tr_loss, s1_arr, s2_arr, w1, w2 = train(epoch)\n","    tr_acc = test_acc(train_loader)\n","    val_acc = test_acc(val_loader)\n","    val_loss = test_loss(val_loader,epoch)\n","    time_elapsed = time.time() - since\n","    print('*====**')\n","    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Epoch: {:03d}, Train Loss: {:.7f}, '\n","          'Train Acc: {:.7f}, Test Loss: {:.7f}, Test Acc: {:.7f}'.format(epoch, tr_loss,\n","                                                       tr_acc, val_loss, val_acc))\n","\n","    writer.add_scalars('Acc',{'train_acc':tr_acc,'val_acc':val_acc},  epoch)\n","    writer.add_scalars('Loss', {'train_loss': tr_loss, 'val_loss': val_loss},  epoch)\n","    writer.add_histogram('Hist/hist_s1', s1_arr, epoch)\n","    writer.add_histogram('Hist/hist_s2', s2_arr, epoch)\n","\n","    if val_loss < best_loss and epoch > 5:\n","        print(\"saving best model\")\n","        best_loss = val_loss\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        if save_model:\n","            torch.save(best_model_wts, os.path.join(opt.save_path,str(fold)+'.pth'))\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 34s\n","Epoch: 000, Train Loss: 1.2411613, Train Acc: 0.4916667, Test Loss: 1.0387416, Test Acc: 0.4650000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 001, Train Loss: 1.3179418, Train Acc: 0.4933333, Test Loss: 1.2478343, Test Acc: 0.5650000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 002, Train Loss: 1.2360230, Train Acc: 0.5000000, Test Loss: 0.9894279, Test Acc: 0.5450000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 31s\n","Epoch: 003, Train Loss: 1.3116309, Train Acc: 0.4783333, Test Loss: 1.3784491, Test Acc: 0.5050000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 31s\n","Epoch: 004, Train Loss: 1.2105162, Train Acc: 0.5050000, Test Loss: 1.2521085, Test Acc: 0.5000000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 005, Train Loss: 1.1902481, Train Acc: 0.4950000, Test Loss: 1.0402542, Test Acc: 0.4650000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 31s\n","Epoch: 006, Train Loss: 1.1280815, Train Acc: 0.5083333, Test Loss: 1.2217204, Test Acc: 0.4850000\n","saving best model\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 007, Train Loss: 1.1232686, Train Acc: 0.5116667, Test Loss: 0.9998550, Test Acc: 0.5300000\n","saving best model\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 008, Train Loss: 1.1104510, Train Acc: 0.5083333, Test Loss: 1.0116299, Test Acc: 0.5150000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 009, Train Loss: 1.0952052, Train Acc: 0.5083333, Test Loss: 0.9973533, Test Acc: 0.5100000\n","saving best model\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 010, Train Loss: 1.0523345, Train Acc: 0.5216667, Test Loss: 0.9934296, Test Acc: 0.5350000\n","saving best model\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 011, Train Loss: 1.0211523, Train Acc: 0.4800000, Test Loss: 0.9995192, Test Acc: 0.5300000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 012, Train Loss: 1.0487526, Train Acc: 0.4966667, Test Loss: 1.0094286, Test Acc: 0.5000000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 013, Train Loss: 1.0288375, Train Acc: 0.5166667, Test Loss: 1.0010059, Test Acc: 0.4700000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 014, Train Loss: 1.0399603, Train Acc: 0.5116667, Test Loss: 1.0097208, Test Acc: 0.5350000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 31s\n","Epoch: 015, Train Loss: 1.0290219, Train Acc: 0.5100000, Test Loss: 0.9987419, Test Acc: 0.5350000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 32s\n","Epoch: 016, Train Loss: 1.0290184, Train Acc: 0.5250000, Test Loss: 0.9944731, Test Acc: 0.5050000\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 017, Train Loss: 1.0147215, Train Acc: 0.5133333, Test Loss: 0.9903692, Test Acc: 0.5000000\n","saving best model\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 018, Train Loss: 1.0110141, Train Acc: 0.4900000, Test Loss: 0.9884407, Test Acc: 0.4700000\n","saving best model\n","train...........\n","LR 0.01\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 019, Train Loss: 0.9982623, Train Acc: 0.5083333, Test Loss: 0.9735538, Test Acc: 0.5350000\n","saving best model\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 020, Train Loss: 0.9737598, Train Acc: 0.5200000, Test Loss: 0.9780792, Test Acc: 0.4400000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 021, Train Loss: 0.9976184, Train Acc: 0.5366667, Test Loss: 1.0074363, Test Acc: 0.4850000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 022, Train Loss: 0.9997769, Train Acc: 0.5450000, Test Loss: 0.9892546, Test Acc: 0.4550000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 023, Train Loss: 0.9901559, Train Acc: 0.5633333, Test Loss: 0.9827820, Test Acc: 0.4600000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 024, Train Loss: 0.9716720, Train Acc: 0.4966667, Test Loss: 0.9829713, Test Acc: 0.4550000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 025, Train Loss: 0.9662700, Train Acc: 0.4933333, Test Loss: 0.9754392, Test Acc: 0.5200000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 026, Train Loss: 0.9813835, Train Acc: 0.4466667, Test Loss: 0.9842459, Test Acc: 0.4450000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 027, Train Loss: 0.9589265, Train Acc: 0.5050000, Test Loss: 1.0093514, Test Acc: 0.4400000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 028, Train Loss: 0.9839523, Train Acc: 0.4850000, Test Loss: 0.9830828, Test Acc: 0.4600000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 029, Train Loss: 0.9743984, Train Acc: 0.5066667, Test Loss: 0.9855249, Test Acc: 0.5550000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 030, Train Loss: 0.9518150, Train Acc: 0.4950000, Test Loss: 0.9968017, Test Acc: 0.4700000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 031, Train Loss: 0.9428100, Train Acc: 0.5183333, Test Loss: 1.0031921, Test Acc: 0.5800000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 032, Train Loss: 0.9413258, Train Acc: 0.4916667, Test Loss: 0.9902664, Test Acc: 0.4450000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 033, Train Loss: 0.9242268, Train Acc: 0.4916667, Test Loss: 1.0788274, Test Acc: 0.4650000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 034, Train Loss: 0.8875817, Train Acc: 0.6416667, Test Loss: 0.9621432, Test Acc: 0.5650000\n","saving best model\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 035, Train Loss: 0.8681141, Train Acc: 0.5400000, Test Loss: 1.0725079, Test Acc: 0.4650000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 036, Train Loss: 0.8219767, Train Acc: 0.6783333, Test Loss: 0.9951661, Test Acc: 0.6250000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 037, Train Loss: 0.8228631, Train Acc: 0.6133333, Test Loss: 1.0872687, Test Acc: 0.4700000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 038, Train Loss: 0.8370376, Train Acc: 0.5116667, Test Loss: 2.1846436, Test Acc: 0.5500000\n","train...........\n","LR 0.005\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 039, Train Loss: 0.8052453, Train Acc: 0.5033333, Test Loss: 1.9827432, Test Acc: 0.5800000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 040, Train Loss: 0.7838504, Train Acc: 0.6800000, Test Loss: 1.0938774, Test Acc: 0.6150000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 041, Train Loss: 0.7378133, Train Acc: 0.8150000, Test Loss: 1.0652369, Test Acc: 0.5700000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 042, Train Loss: 0.7358432, Train Acc: 0.7033333, Test Loss: 1.1287565, Test Acc: 0.6100000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 043, Train Loss: 0.7187336, Train Acc: 0.6283333, Test Loss: 1.3526329, Test Acc: 0.4600000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 044, Train Loss: 0.7127574, Train Acc: 0.5500000, Test Loss: 1.5370535, Test Acc: 0.5550000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 045, Train Loss: 0.7091847, Train Acc: 0.5083333, Test Loss: 1.9097472, Test Acc: 0.4600000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 046, Train Loss: 0.7008543, Train Acc: 0.5233333, Test Loss: 2.2027597, Test Acc: 0.5350000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 047, Train Loss: 0.6775801, Train Acc: 0.6766667, Test Loss: 1.4398825, Test Acc: 0.4800000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 048, Train Loss: 0.6586621, Train Acc: 0.7383333, Test Loss: 1.2153899, Test Acc: 0.5450000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 049, Train Loss: 0.6376678, Train Acc: 0.8550000, Test Loss: 1.2007447, Test Acc: 0.5800000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 31s\n","Epoch: 050, Train Loss: 0.6742986, Train Acc: 0.5683333, Test Loss: 1.9337441, Test Acc: 0.5400000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 051, Train Loss: 0.6600823, Train Acc: 0.5616667, Test Loss: 1.7776971, Test Acc: 0.5700000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 052, Train Loss: 0.6560520, Train Acc: 0.5616667, Test Loss: 1.7029260, Test Acc: 0.4650000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 053, Train Loss: 0.6405961, Train Acc: 0.5683333, Test Loss: 1.8364804, Test Acc: 0.5550000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 054, Train Loss: 0.6300571, Train Acc: 0.5283333, Test Loss: 1.7415525, Test Acc: 0.4500000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 055, Train Loss: 0.6420341, Train Acc: 0.7733333, Test Loss: 1.3502719, Test Acc: 0.5500000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 056, Train Loss: 0.6227287, Train Acc: 0.7300000, Test Loss: 1.3664204, Test Acc: 0.4900000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 057, Train Loss: 0.6135741, Train Acc: 0.7166667, Test Loss: 1.3105036, Test Acc: 0.5450000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 058, Train Loss: 0.6107661, Train Acc: 0.6233333, Test Loss: 1.9514753, Test Acc: 0.5500000\n","train...........\n","LR 0.0025\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 059, Train Loss: 0.5947420, Train Acc: 0.5866667, Test Loss: 1.6268576, Test Acc: 0.6200000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 060, Train Loss: 0.5787454, Train Acc: 0.5683333, Test Loss: 3.6082864, Test Acc: 0.5700000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 061, Train Loss: 0.5914199, Train Acc: 0.7500000, Test Loss: 1.5504038, Test Acc: 0.4950000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 31s\n","Epoch: 062, Train Loss: 0.6077783, Train Acc: 0.6450000, Test Loss: 1.7378468, Test Acc: 0.4550000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 063, Train Loss: 0.5557101, Train Acc: 0.6066667, Test Loss: 2.6443870, Test Acc: 0.5600000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 064, Train Loss: 0.5543506, Train Acc: 0.9416667, Test Loss: 1.4196985, Test Acc: 0.5850000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 065, Train Loss: 0.5426783, Train Acc: 0.7666667, Test Loss: 1.6405372, Test Acc: 0.5150000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 066, Train Loss: 0.5447075, Train Acc: 0.8550000, Test Loss: 1.6240646, Test Acc: 0.5400000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 067, Train Loss: 0.5225304, Train Acc: 0.7183333, Test Loss: 1.6246601, Test Acc: 0.5050000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 068, Train Loss: 0.5348887, Train Acc: 0.8866667, Test Loss: 1.6703987, Test Acc: 0.5850000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 069, Train Loss: 0.5335177, Train Acc: 0.9583333, Test Loss: 1.5650808, Test Acc: 0.6050000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 070, Train Loss: 0.5148489, Train Acc: 0.8583333, Test Loss: 1.5838437, Test Acc: 0.5600000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 071, Train Loss: 0.5045336, Train Acc: 0.8750000, Test Loss: 1.7310535, Test Acc: 0.5900000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 072, Train Loss: 0.5212082, Train Acc: 0.7833333, Test Loss: 1.5764270, Test Acc: 0.5850000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 073, Train Loss: 0.5148542, Train Acc: 0.5750000, Test Loss: 3.0213420, Test Acc: 0.5500000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 074, Train Loss: 0.5279729, Train Acc: 0.9416667, Test Loss: 1.6819282, Test Acc: 0.5800000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 23s\n","Epoch: 075, Train Loss: 0.5223763, Train Acc: 0.8350000, Test Loss: 1.5039413, Test Acc: 0.5700000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 076, Train Loss: 0.4831404, Train Acc: 0.9466667, Test Loss: 1.6750177, Test Acc: 0.5900000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 077, Train Loss: 0.5405887, Train Acc: 0.9283333, Test Loss: 1.6311551, Test Acc: 0.5950000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 078, Train Loss: 0.5067881, Train Acc: 0.6983333, Test Loss: 2.2485272, Test Acc: 0.5750000\n","train...........\n","LR 0.00125\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 079, Train Loss: 0.5119879, Train Acc: 0.6800000, Test Loss: 1.7099902, Test Acc: 0.5150000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 080, Train Loss: 0.5056207, Train Acc: 0.8566667, Test Loss: 1.6061293, Test Acc: 0.5850000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 081, Train Loss: 0.4545450, Train Acc: 0.7900000, Test Loss: 2.1889334, Test Acc: 0.5800000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 23s\n","Epoch: 082, Train Loss: 0.4939771, Train Acc: 0.8566667, Test Loss: 1.8693070, Test Acc: 0.5700000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 23s\n","Epoch: 083, Train Loss: 0.4719769, Train Acc: 0.8583333, Test Loss: 1.7343614, Test Acc: 0.5600000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 084, Train Loss: 0.4789204, Train Acc: 0.9400000, Test Loss: 1.8587071, Test Acc: 0.5750000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 085, Train Loss: 0.4858705, Train Acc: 0.8950000, Test Loss: 1.9698346, Test Acc: 0.5950000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 086, Train Loss: 0.4685999, Train Acc: 0.9683333, Test Loss: 1.8177804, Test Acc: 0.5700000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 087, Train Loss: 0.4773590, Train Acc: 0.9150000, Test Loss: 1.8077891, Test Acc: 0.5700000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 088, Train Loss: 0.4592931, Train Acc: 0.9700000, Test Loss: 1.8691333, Test Acc: 0.5900000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 089, Train Loss: 0.4620474, Train Acc: 0.9483333, Test Loss: 1.7339220, Test Acc: 0.5750000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 090, Train Loss: 0.4528683, Train Acc: 0.9133333, Test Loss: 1.8953303, Test Acc: 0.5750000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 24s\n","Epoch: 091, Train Loss: 0.4604910, Train Acc: 0.9816667, Test Loss: 1.8769441, Test Acc: 0.5800000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 092, Train Loss: 0.4430478, Train Acc: 0.9666667, Test Loss: 1.8992714, Test Acc: 0.5800000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 23s\n","Epoch: 093, Train Loss: 0.4547774, Train Acc: 0.9700000, Test Loss: 1.8584581, Test Acc: 0.5900000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 094, Train Loss: 0.4602182, Train Acc: 0.9783333, Test Loss: 1.8730380, Test Acc: 0.5850000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 095, Train Loss: 0.4511967, Train Acc: 0.8883333, Test Loss: 1.8304277, Test Acc: 0.5650000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 096, Train Loss: 0.4233388, Train Acc: 0.9850000, Test Loss: 1.9700666, Test Acc: 0.5950000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 30s\n","Epoch: 097, Train Loss: 0.4474204, Train Acc: 0.9050000, Test Loss: 1.8443074, Test Acc: 0.5850000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 098, Train Loss: 0.4170034, Train Acc: 0.9366667, Test Loss: 1.9861432, Test Acc: 0.5650000\n","train...........\n","LR 0.000625\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 099, Train Loss: 0.4257806, Train Acc: 0.9383333, Test Loss: 1.9840667, Test Acc: 0.5950000\n"]}]},{"cell_type":"markdown","metadata":{"id":"LmEODGTbqZep"},"source":["## Testing\n","Confusion Matrix is produced for the test data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FE8DzXr7V-RH","executionInfo":{"elapsed":10048,"status":"ok","timestamp":1637474541429,"user":{"displayName":"Arun Talkad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi90KExK9fe7Qcd7jFvDxW6c5tPpI_Xw89GqJnDuw=s64","userId":"10224658465974025190"},"user_tz":480},"outputId":"0c7d9aae-cae7-4909-be19-429836c9bf59"},"source":["model = Network(opt.indim,opt.ratio,opt.nclass).to(device)\n","model.load_state_dict(torch.load(os.path.join(opt.save_path,str(fold)+'.pth')))\n","model.eval()\n","preds = []\n","trues = []\n","correct = 0\n","for data in test_loader:\n","  data = data.to(device)\n","  outputs= model(data.x, data.edge_index, data.batch, data.edge_attr,data.pos)\n","  pred = outputs[0].max(1)[1] \n","  preds.append(pred.cpu().detach().numpy())\n","  trues.append(data.y.cpu().detach().numpy())\n","  correct += pred.eq(data.y).sum().item()\n","preds = np.concatenate(preds,axis=0)\n","trues = np.concatenate(trues, axis=0)\n","cm = confusion_matrix(trues,preds)\n","print(\"Confusion matrix\")\n","print(classification_report(trues, preds))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion matrix\n","              precision    recall  f1-score   support\n","\n","           0       0.53      0.84      0.65       125\n","           1       0.44      0.15      0.22       109\n","\n","    accuracy                           0.52       234\n","   macro avg       0.49      0.49      0.44       234\n","weighted avg       0.49      0.52      0.45       234\n","\n"]}]}]}