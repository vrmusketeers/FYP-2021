{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PRGNN.ipynb","provenance":[{"file_id":"1vzYxkzEPEI4-X4iJbZvoEWU4FTTCZSco","timestamp":1637464504278}],"collapsed_sections":[],"mount_file_id":"1Z6EbqILG8dqYWnwt_SKG1thjXKMrbbx6","authorship_tag":"ABX9TyP8EPSTWs505u3SbDktQNna"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JCeBOf3Mi4Cm"},"source":["## Pooling Regularized Graph Neural Network\n","The goal here is to model is to extend the BrainGNN algorithm by introducing regularization terms for ranking-based pooling methods to ensure better node selection and provide flexibility between individual-level and group-level. In both the cases the output layer is a softmax function to obtain binary classification."]},{"cell_type":"markdown","metadata":{"id":"DXy0ogSXk1TM"},"source":["## Install Libraries"]},{"cell_type":"code","metadata":{"id":"VcEAe46LU1YO"},"source":["!pip install nilearn\n","!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n","!pip install torch-geometric\n","!pip install deepdish\n","!pip install tensorboardX"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMM9UG8kk3Xq"},"source":["## Download custom PRGNN implementation and the source code "]},{"cell_type":"code","metadata":{"id":"NyNTkbUW92Uy"},"source":["! git clone https://github.com/vrmusketeers/FYP-2021.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4E0bjyed8ajf"},"source":["import sys\n","sys.path.append('/content/FYP-2021/notebooks/graph_models/prgnn')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXcLAxru1l0i"},"source":["import os\n","import numpy as np\n","import argparse\n","import time\n","import copy\n","\n","import matplotlib.pyplot as plt\n","import deepdish as dd\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","from tensorboardX import SummaryWriter\n","\n","from BiopointData import BiopointDataset\n","from torch_geometric.data import DataLoader\n","from net.brain_networks import LI_Net,NNGAT_Net\n","\n","from utils.utils import normal_transform_train,normal_transform_test,train_val_test_split\n","from utils.mmd_loss import MMD_loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hf5bhJBCpggL"},"source":["## Set parameters\n","The parameters are set to run the model with Adam Optimizer for 100 Epochs. The model is a GNN built using pytorch-geometric"]},{"cell_type":"code","metadata":{"id":"SU835PhL6mbP"},"source":["torch.manual_seed(123)\n","\n","EPS = 1e-10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uA463hnT6qpy"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--epoch', type=int, default=1, help='starting epoch')\n","parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs of training')\n","parser.add_argument('--batchSize', type=int, default=100, help='size of the batches')\n","parser.add_argument('--dataroot', type=str, default='/content/data/ABIDE_pcp/cpac/filt_noglobal', help='Dataset Root')\n","parser.add_argument('--fold', type=int, default=1, help='training which fold')\n","parser.add_argument('--lr', type = float, default=0.01, help='learning rate')\n","parser.add_argument('--rep', type=int, default=30, help='augmentation times')\n","parser.add_argument('--stepsize', type=int, default=20, help='scheduler step size')\n","parser.add_argument('--gamma', type=float, default=0.5, help='scheduler shrinking rate')\n","parser.add_argument('--weightdecay', type=float, default=5e-2, help='regularization')\n","parser.add_argument('--lamb0', type=float, default=1, help='classification loss weight')\n","parser.add_argument('--lamb1', type=float, default=1, help='s1 unit regularization')\n","parser.add_argument('--lamb2', type=float, default=1, help='s2 unit regularization')\n","parser.add_argument('--lamb3', type=float, default=0.1, help='s1 distance regularization')\n","parser.add_argument('--lamb4', type=float, default=0.1, help='s2 distance regularization')\n","parser.add_argument('--lamb5', type=float, default=0, help='s1 consistence regularization')\n","parser.add_argument('--lamb6', type=float, default=0, help='s2 consistence regularization')\n","parser.add_argument('--distL', type=str, default='bce', help='bce || mmd')\n","parser.add_argument('--poolmethod', type=str, default='topk', help='topk || sag')\n","parser.add_argument('--optimizer', type=str, default='Adam', help='Adam || SGD')\n","parser.add_argument('--layer', type=int, default=2, help='number of GNN layers')\n","parser.add_argument('--nodes', type=int, default=84, help='number of nodes')\n","parser.add_argument('--ratio', type=float, default=0.5, help='pooling ratio')\n","parser.add_argument('--net', type=str, default='NNGAT', help='model name: NNGAT || LI_NET')\n","parser.add_argument('--indim', type=int, default=200, help='feature dim')\n","parser.add_argument('--nclass', type=int, default=2, help='feature dim')\n","parser.add_argument('--save_model', action='store_true')\n","parser.add_argument('--normalization', action='store_true')\n","parser.set_defaults(save_model=True)\n","parser.set_defaults(normalization=True)\n","parser.add_argument('-f')\n","opt = parser.parse_args()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SDejAh6Vp0Hq"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"6CCoO6Vq61UA"},"source":["#################### Parameter Initialization #######################\n","name = 'Biopoint'\n","writer = SummaryWriter(os.path.join('./log/{}_fold{}_consis{}'.format(opt.net,opt.fold,opt.lamb5)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DbNJDDqerog5"},"source":["############# Define Dataloader -- need costumize#####################\n","dataset = BiopointDataset(opt.dataroot, name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5NudWmnDnehH"},"source":["dataset.data.y = dataset.data.y.squeeze()\n","dataset.data.x[dataset.data.x == float('inf')] = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2WdIvYQ64BI","executionInfo":{"status":"ok","timestamp":1638152239081,"user_tz":480,"elapsed":1637,"user":{"displayName":"Arun Talkad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi90KExK9fe7Qcd7jFvDxW6c5tPpI_Xw89GqJnDuw=s64","userId":"10224658465974025190"}},"outputId":"b9fb4b46-fd6a-4208-a2be-67f55129ed2e"},"source":["# ######################## Data Preprocessing ########################\n","dataset = dataset.shuffle()\n","\n","train_dataset = dataset[:600]\n","val_dataset = dataset[600:800]\n","test_dataset = dataset[800:1034]\n","\n","# ###################### Normalize features ##########################\n","if opt.normalization:\n","    for i in range(train_dataset.data.x.shape[1]):\n","        train_dataset.data.x[:, i], lamb, xmean, xstd = normal_transform_train(train_dataset.data.x[:, i])\n","        test_dataset.data.x[:, i] = normal_transform_test(test_dataset.data.x[:, i],lamb, xmean, xstd)\n","        val_dataset.data.x[:, i] = normal_transform_test(val_dataset.data.x[:, i], lamb, xmean, xstd)\n","\n","train_loader = DataLoader(train_dataset,batch_size=opt.batchSize, shuffle = True)\n","test_loader = DataLoader(test_dataset,batch_size=opt.batchSize,shuffle = False)\n","val_loader = DataLoader(val_dataset, batch_size=opt.batchSize, shuffle = False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}]},{"cell_type":"markdown","metadata":{"id":"5oVdw0nyp3sv"},"source":["## The model \n","A Graph Convolutional Layer with TopKPooling, Batch Normalization layers and a Linear Output Layer that produces only binary results is deviced."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7erwK5vPvRWl","executionInfo":{"status":"ok","timestamp":1638152239555,"user_tz":480,"elapsed":5,"user":{"displayName":"Arun Talkad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi90KExK9fe7Qcd7jFvDxW6c5tPpI_Xw89GqJnDuw=s64","userId":"10224658465974025190"}},"outputId":"f4e4159f-0db3-4fbd-e395-44975465e364"},"source":["############### Define Graph Deep Learning Network ##########################\n","if opt.net =='LI_NET':\n","    model = LI_Net(opt.ratio).to(device)\n","elif opt.net == 'NNGAT':\n","    model = NNGAT_Net(opt.ratio, indim=opt.indim, poolmethod = opt.poolmethod).to(device)\n","\n","\n","print(model)\n","if opt.optimizer == 'Adam':\n","    optimizer = torch.optim.Adam(model.parameters(), lr= opt.lr, weight_decay=opt.weightdecay)\n","elif opt.optimizer == 'SGD':\n","    optimizer = torch.optim.SGD(model.parameters(), lr =opt.lr, momentum = 0.9, weight_decay=opt.weightdecay, nesterov = True)\n","\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.stepsize, gamma=opt.gamma)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NNGAT_Net(\n","  (conv1): GATConv(200, 32, heads=1)\n","  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool1): TopKPooling(32, ratio=0.5, multiplier=1)\n","  (conv2): GATConv(32, 32, heads=1)\n","  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool2): TopKPooling(32, ratio=0.5, multiplier=1)\n","  (fc1): Linear(in_features=128, out_features=32, bias=True)\n","  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc2): Linear(in_features=32, out_features=8, bias=True)\n","  (bn5): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc3): Linear(in_features=8, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"FkBFR3puqNhK"},"source":["## Loss Functions"]},{"cell_type":"code","metadata":{"id":"69PghNy5I6-4"},"source":["############################### Define Other Loss Functions ########################################\n","if opt.distL == 'bce':\n","    ###### bce loss\n","    def dist_loss(s,ratio):\n","        if ratio > 0.5:\n","            ratio = 1-ratio\n","        s = s.sort(dim=1).values\n","        res =  -torch.log(s[:,-int(s.size(1)*ratio):]+EPS).mean() -torch.log(1-s[:,:int(s.size(1)*ratio)]+EPS).mean()\n","        return res\n","\n","elif opt.distL == 'mmd':\n","    ######## mmd\n","    mmd = MMD_loss()\n","    def dist_loss(s,ratio):\n","        s = s.sort(dim=1).values\n","        source = s[:,-int(s.size(1)*ratio):]\n","        target = s[:,:int(s.size(1)*ratio)]\n","        res = mmd(source,target)\n","        return -res\n","\n","def consist_loss(s):\n","    if len(s) == 0:\n","        return 0\n","    else:\n","        s = torch.sigmoid(s)\n","        W = torch.ones(s.shape[0],s.shape[0])\n","        D = torch.eye(s.shape[0])*torch.sum(W,dim=1)\n","        L = D-W\n","        L = L.to(device)\n","        res = torch.trace(torch.transpose(s,0,1) @ L @ s)/(s.shape[0]*s.shape[0])\n","        return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkekMkvDI8A3"},"source":["###################### Network Training Function#####################################\n","def train(epoch):\n","    print('train...........')\n","    model.train()\n","\n","    s1_list = []\n","    s2_list = []\n","    loss_all = 0\n","    loss_en1_all  = 0\n","    loss_en2_all = 0\n","\n","    i = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","\n","        output, s1, s2 = model(data.x, data.edge_index, data.batch, data.edge_attr)\n","\n","        s1_list.append(s1.view(-1).detach().cpu().numpy())\n","        s2_list.append(s2.view(-1).detach().cpu().numpy())\n","\n","        loss_c = F.nll_loss(output, data.y) # classification loss\n","\n","        loss_dist1 = dist_loss(s1, opt.ratio)\n","        loss_dist2 = dist_loss(s2, opt.ratio)\n","        loss_consist = consist_loss(s1[data.y == 1]) + consist_loss(s1[data.y == 0])\n","        loss = opt.lamb0 * loss_c \\\n","               + opt.lamb3 * loss_dist1 + opt.lamb4 * loss_dist2 + opt.lamb5 * loss_consist\n","        writer.add_scalar('train/classification_loss', loss_c, epoch * len(train_loader) + i)\n","        writer.add_scalar('train/entropy_loss1', loss_dist1, epoch * len(train_loader) + i)\n","        writer.add_scalar('train/entropy_loss2', loss_dist2, epoch * len(train_loader) + i)\n","        writer.add_scalar('train/consistance_loss', loss_consist, epoch * len(train_loader) + i)\n","\n","        i = i + 1\n","\n","        loss.backward()\n","        loss_all += loss.item() * data.num_graphs\n","        loss_en1_all +=loss_dist1.item() *data.num_graphs\n","        loss_en2_all += loss_dist2.item() * data.num_graphs\n","        optimizer.step()\n","        scheduler.step()\n","\n","        s1_arr = np.hstack(s1_list)\n","        s2_arr = np.hstack(s2_list)\n","\n","        if not os.path.exists('outputs/'):\n","            os.makedirs('outputs/')\n","        if epoch%5 == 0:\n","            dd.io.save(\n","                'outputs/train_s1_{}_epoch{}_dist{}_cnsis{}_pool{}.h5'.format(opt.net, epoch, opt.lamb3, opt.lamb5,\n","                                                                              opt.ratio), {'s1': s1_arr})\n","            dd.io.save(\n","                'outputs/train_s2_{}_epoch{}_dist{}_cnsis{}_pool{}.h5'.format(opt.net, epoch, opt.lamb3, opt.lamb5,\n","                                                                              opt.ratio), {'s2': s1_arr})\n","\n","    return loss_all / len(train_dataset), s1_arr, s2_arr, loss_en1_all / len(train_dataset),loss_en2_all / len(train_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDY2VZyII-cP"},"source":["###################### Network Testing Function#####################################\n","def test_acc(loader):\n","    model.eval()\n","    correct = 0\n","    for data in loader:\n","        data = data.to(device)\n","        output,_,_= model(data.x, data.edge_index, data.batch, data.edge_attr)\n","        pred = output.max(dim=1)[1]\n","        correct += pred.eq(data.y).sum().item()\n","    return correct / len(loader.dataset)\n","\n","def test_loss(loader,epoch):\n","    print('testing...........')\n","    model.eval()\n","    loss_all = 0\n","\n","    i=0\n","    for data in loader:\n","        data = data.to(device)\n","        output,s1,s2 = model(data.x, data.edge_index, data.batch, data.edge_attr)\n","        loss_c = F.nll_loss(output, data.y)\n","\n","        loss_dist1 = dist_loss(s1, opt.ratio)\n","        loss_dist2 = dist_loss(s2, opt.ratio)\n","        loss_consist = consist_loss(s1)\n","        loss = opt.lamb0 * loss_c \\\n","               + opt.lamb3 * loss_dist1 + opt.lamb4 * loss_dist2 + opt.lamb5 * loss_consist\n","        writer.add_scalar('val/classification_loss', loss_c, epoch * len(loader) + i)\n","        writer.add_scalar('val/entropy_loss1', loss_dist1, epoch * len(loader) + i)\n","        writer.add_scalar('val/entropy_loss2', loss_dist2, epoch * len(loader) + i)\n","        writer.add_scalar('val/consistance_loss', loss_consist, epoch * len(loader) + i)\n","        i = i + 1\n","\n","        loss_all += loss.item() * data.num_graphs\n","    return loss_all / len(loader.dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gj1a2Jm4qr_I"},"source":["## Training\n","The model is trained and the best model stored for future use"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1yZ7936JOf2","executionInfo":{"status":"ok","timestamp":1638161205150,"user_tz":480,"elapsed":2082944,"user":{"displayName":"Arun Talkad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi90KExK9fe7Qcd7jFvDxW6c5tPpI_Xw89GqJnDuw=s64","userId":"10224658465974025190"}},"outputId":"f50dbdb6-aaf3-45c8-951c-79042786d799"},"source":["#######################################################################################\n","############################   Model Training #########################################\n","#######################################################################################\n","best_model_wts = copy.deepcopy(model.state_dict())\n","best_loss = 1e10\n","for epoch in range(0, opt.n_epochs):\n","    since  = time.time()\n","    tr_loss, s1_arr, s2_arr,le1,le2 = train(epoch)\n","    tr_acc = test_acc(train_loader)\n","    val_acc = test_acc(val_loader)\n","    val_loss = test_loss(val_loader,epoch)\n","    time_elapsed = time.time() - since\n","    print('*====**')\n","    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Epoch: {:03d}, Train Loss: {:.7f}, '\n","          'Train Acc: {:.7f}, Test Loss: {:.7f}, Test Acc: {:.7f}'.format(epoch, tr_loss,\n","                                                       tr_acc, val_loss, val_acc))\n","\n","    writer.add_scalars('Acc',{'train_acc':tr_acc,'val_acc':val_acc},  epoch)\n","    writer.add_scalars('Loss', {'train_loss': tr_loss, 'val_loss': val_loss},  epoch)\n","    writer.add_scalar('Ent/ent1', le1, epoch)\n","    writer.add_scalar('Ent/ent2', le2, epoch)\n","    writer.add_histogram('Hist/hist_s1', s1_arr, epoch)\n","    writer.add_histogram('Hist/hist_s2', s2_arr, epoch)\n","\n","\n","    if val_loss < best_loss and epoch > 5:\n","        print(\"saving best model\")\n","        best_loss = val_loss\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        if not os.path.exists('models/'):\n","            os.makedirs('models/')\n","        if opt.save_model:\n","            torch.save(best_model_wts,\n","                       'models/rep{}_biopoint_{}_{}_{}.pth'.format(opt.rep,opt.fold,opt.net,opt.lamb5))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train...........\n","testing...........\n","*====**\n","1m 32s\n","Epoch: 000, Train Loss: 0.9837202, Train Acc: 0.5416667, Test Loss: 0.9625081, Test Acc: 0.5250000\n","train...........\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 001, Train Loss: 0.9846769, Train Acc: 0.5616667, Test Loss: 0.9703986, Test Acc: 0.5300000\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 002, Train Loss: 0.9731723, Train Acc: 0.5500000, Test Loss: 0.9731568, Test Acc: 0.4450000\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 003, Train Loss: 0.9765731, Train Acc: 0.5566667, Test Loss: 0.9716662, Test Acc: 0.4600000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 004, Train Loss: 0.9634687, Train Acc: 0.5533333, Test Loss: 0.9710906, Test Acc: 0.4600000\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 005, Train Loss: 0.9613514, Train Acc: 0.5533333, Test Loss: 0.9688110, Test Acc: 0.4700000\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 006, Train Loss: 0.9707504, Train Acc: 0.5700000, Test Loss: 0.9699408, Test Acc: 0.4550000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 007, Train Loss: 0.9672472, Train Acc: 0.5750000, Test Loss: 0.9677345, Test Acc: 0.4300000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 008, Train Loss: 0.9626601, Train Acc: 0.5566667, Test Loss: 0.9643183, Test Acc: 0.4650000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 009, Train Loss: 0.9619345, Train Acc: 0.5866667, Test Loss: 0.9646043, Test Acc: 0.4650000\n","train...........\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 010, Train Loss: 0.9535688, Train Acc: 0.5850000, Test Loss: 0.9649957, Test Acc: 0.4550000\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 011, Train Loss: 0.9550726, Train Acc: 0.5850000, Test Loss: 0.9631530, Test Acc: 0.4900000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 012, Train Loss: 0.9490457, Train Acc: 0.5866667, Test Loss: 0.9592377, Test Acc: 0.5200000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 013, Train Loss: 0.9474636, Train Acc: 0.5883333, Test Loss: 0.9575496, Test Acc: 0.5350000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 014, Train Loss: 0.9400173, Train Acc: 0.6200000, Test Loss: 0.9562350, Test Acc: 0.5500000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 28s\n","Epoch: 015, Train Loss: 0.9407280, Train Acc: 0.6350000, Test Loss: 0.9571501, Test Acc: 0.5200000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 016, Train Loss: 0.9301669, Train Acc: 0.6283333, Test Loss: 0.9576159, Test Acc: 0.5000000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 017, Train Loss: 0.9322751, Train Acc: 0.6383333, Test Loss: 0.9554006, Test Acc: 0.5050000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 018, Train Loss: 0.9278815, Train Acc: 0.6416667, Test Loss: 0.9539254, Test Acc: 0.5100000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 019, Train Loss: 0.9243275, Train Acc: 0.6466667, Test Loss: 0.9533576, Test Acc: 0.5300000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 020, Train Loss: 0.9263788, Train Acc: 0.6483333, Test Loss: 0.9526244, Test Acc: 0.5400000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 021, Train Loss: 0.9183245, Train Acc: 0.6550000, Test Loss: 0.9505006, Test Acc: 0.5600000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 022, Train Loss: 0.9183552, Train Acc: 0.6666667, Test Loss: 0.9476505, Test Acc: 0.5650000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 023, Train Loss: 0.9140231, Train Acc: 0.6683333, Test Loss: 0.9463085, Test Acc: 0.5700000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 024, Train Loss: 0.9212452, Train Acc: 0.6700000, Test Loss: 0.9467781, Test Acc: 0.5800000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 025, Train Loss: 0.9133672, Train Acc: 0.6716667, Test Loss: 0.9468461, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 026, Train Loss: 0.9203162, Train Acc: 0.6750000, Test Loss: 0.9465410, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 027, Train Loss: 0.9118692, Train Acc: 0.6750000, Test Loss: 0.9465894, Test Acc: 0.5800000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 028, Train Loss: 0.9155249, Train Acc: 0.6733333, Test Loss: 0.9465464, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 029, Train Loss: 0.9056086, Train Acc: 0.6816667, Test Loss: 0.9467948, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 030, Train Loss: 0.9075526, Train Acc: 0.6816667, Test Loss: 0.9465198, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 031, Train Loss: 0.9095496, Train Acc: 0.6800000, Test Loss: 0.9463579, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 032, Train Loss: 0.9128672, Train Acc: 0.6800000, Test Loss: 0.9463134, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 033, Train Loss: 0.9090937, Train Acc: 0.6833333, Test Loss: 0.9459574, Test Acc: 0.5650000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 034, Train Loss: 0.9078464, Train Acc: 0.6816667, Test Loss: 0.9459712, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 035, Train Loss: 0.9067365, Train Acc: 0.6833333, Test Loss: 0.9459287, Test Acc: 0.5650000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 036, Train Loss: 0.9134919, Train Acc: 0.6866667, Test Loss: 0.9458829, Test Acc: 0.5650000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 037, Train Loss: 0.9077891, Train Acc: 0.6866667, Test Loss: 0.9457280, Test Acc: 0.5700000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 038, Train Loss: 0.9087093, Train Acc: 0.6883333, Test Loss: 0.9458309, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 039, Train Loss: 0.9073781, Train Acc: 0.6883333, Test Loss: 0.9459532, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 31s\n","Epoch: 040, Train Loss: 0.9170266, Train Acc: 0.6883333, Test Loss: 0.9460046, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 29s\n","Epoch: 041, Train Loss: 0.9103777, Train Acc: 0.6850000, Test Loss: 0.9459282, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 042, Train Loss: 0.9107020, Train Acc: 0.6866667, Test Loss: 0.9458268, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 043, Train Loss: 0.9089344, Train Acc: 0.6866667, Test Loss: 0.9457583, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 044, Train Loss: 0.9131230, Train Acc: 0.6883333, Test Loss: 0.9458844, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 045, Train Loss: 0.9101141, Train Acc: 0.6883333, Test Loss: 0.9459897, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 046, Train Loss: 0.9162466, Train Acc: 0.6866667, Test Loss: 0.9459445, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 047, Train Loss: 0.9073492, Train Acc: 0.6883333, Test Loss: 0.9458331, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 048, Train Loss: 0.9099189, Train Acc: 0.6883333, Test Loss: 0.9456913, Test Acc: 0.5700000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 049, Train Loss: 0.9081518, Train Acc: 0.6866667, Test Loss: 0.9457601, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 050, Train Loss: 0.9098596, Train Acc: 0.6883333, Test Loss: 0.9457154, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 051, Train Loss: 0.9037678, Train Acc: 0.6883333, Test Loss: 0.9458243, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 052, Train Loss: 0.9062045, Train Acc: 0.6850000, Test Loss: 0.9459265, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 053, Train Loss: 0.9137611, Train Acc: 0.6866667, Test Loss: 0.9460125, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 054, Train Loss: 0.9058202, Train Acc: 0.6866667, Test Loss: 0.9460450, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 055, Train Loss: 0.9088020, Train Acc: 0.6883333, Test Loss: 0.9459013, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 056, Train Loss: 0.9099647, Train Acc: 0.6850000, Test Loss: 0.9456484, Test Acc: 0.5650000\n","saving best model\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 057, Train Loss: 0.9104099, Train Acc: 0.6866667, Test Loss: 0.9458806, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 058, Train Loss: 0.9122457, Train Acc: 0.6866667, Test Loss: 0.9458331, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 059, Train Loss: 0.9091833, Train Acc: 0.6866667, Test Loss: 0.9459949, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 060, Train Loss: 0.9036606, Train Acc: 0.6866667, Test Loss: 0.9461079, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 061, Train Loss: 0.9101428, Train Acc: 0.6883333, Test Loss: 0.9458763, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 062, Train Loss: 0.9031184, Train Acc: 0.6866667, Test Loss: 0.9460041, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 063, Train Loss: 0.9043309, Train Acc: 0.6883333, Test Loss: 0.9458521, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 064, Train Loss: 0.9124706, Train Acc: 0.6850000, Test Loss: 0.9462292, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 065, Train Loss: 0.9084553, Train Acc: 0.6850000, Test Loss: 0.9461339, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 066, Train Loss: 0.9087625, Train Acc: 0.6866667, Test Loss: 0.9459016, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 067, Train Loss: 0.9101162, Train Acc: 0.6850000, Test Loss: 0.9460881, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 068, Train Loss: 0.9125866, Train Acc: 0.6883333, Test Loss: 0.9461447, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 069, Train Loss: 0.9064353, Train Acc: 0.6883333, Test Loss: 0.9459431, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 070, Train Loss: 0.9059386, Train Acc: 0.6883333, Test Loss: 0.9460155, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 071, Train Loss: 0.9114091, Train Acc: 0.6883333, Test Loss: 0.9460615, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 072, Train Loss: 0.9104383, Train Acc: 0.6883333, Test Loss: 0.9461409, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 073, Train Loss: 0.9035320, Train Acc: 0.6866667, Test Loss: 0.9461292, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 074, Train Loss: 0.9070513, Train Acc: 0.6883333, Test Loss: 0.9460464, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 075, Train Loss: 0.9025372, Train Acc: 0.6866667, Test Loss: 0.9461454, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 076, Train Loss: 0.9116732, Train Acc: 0.6850000, Test Loss: 0.9460571, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 077, Train Loss: 0.9099136, Train Acc: 0.6850000, Test Loss: 0.9460338, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 078, Train Loss: 0.9058239, Train Acc: 0.6850000, Test Loss: 0.9459655, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 079, Train Loss: 0.9075908, Train Acc: 0.6883333, Test Loss: 0.9459415, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 080, Train Loss: 0.9161119, Train Acc: 0.6883333, Test Loss: 0.9457507, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 081, Train Loss: 0.9066480, Train Acc: 0.6883333, Test Loss: 0.9458952, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 082, Train Loss: 0.9075501, Train Acc: 0.6883333, Test Loss: 0.9457766, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 083, Train Loss: 0.9047207, Train Acc: 0.6850000, Test Loss: 0.9459839, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 084, Train Loss: 0.9083490, Train Acc: 0.6850000, Test Loss: 0.9459892, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 085, Train Loss: 0.9073578, Train Acc: 0.6800000, Test Loss: 0.9461837, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 086, Train Loss: 0.9087815, Train Acc: 0.6833333, Test Loss: 0.9460940, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 087, Train Loss: 0.9080050, Train Acc: 0.6866667, Test Loss: 0.9459705, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 088, Train Loss: 0.9126732, Train Acc: 0.6883333, Test Loss: 0.9457885, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 089, Train Loss: 0.9141135, Train Acc: 0.6816667, Test Loss: 0.9459316, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 090, Train Loss: 0.9106229, Train Acc: 0.6883333, Test Loss: 0.9460420, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 091, Train Loss: 0.9043041, Train Acc: 0.6900000, Test Loss: 0.9459254, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 092, Train Loss: 0.9058681, Train Acc: 0.6850000, Test Loss: 0.9459226, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 093, Train Loss: 0.9108236, Train Acc: 0.6866667, Test Loss: 0.9459369, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 094, Train Loss: 0.9114304, Train Acc: 0.6883333, Test Loss: 0.9457191, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 27s\n","Epoch: 095, Train Loss: 0.9047949, Train Acc: 0.6883333, Test Loss: 0.9458063, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 096, Train Loss: 0.9055835, Train Acc: 0.6883333, Test Loss: 0.9459770, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 26s\n","Epoch: 097, Train Loss: 0.9100563, Train Acc: 0.6866667, Test Loss: 0.9459264, Test Acc: 0.5650000\n","train...........\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 098, Train Loss: 0.9051046, Train Acc: 0.6883333, Test Loss: 0.9459566, Test Acc: 0.5700000\n","train...........\n","testing...........\n","*====**\n","1m 25s\n","Epoch: 099, Train Loss: 0.9063845, Train Acc: 0.6866667, Test Loss: 0.9459864, Test Acc: 0.5700000\n"]}]},{"cell_type":"markdown","metadata":{"id":"LmEODGTbqZep"},"source":["## Testing\n","Test Accuracy is calculated for the test split"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29VpdH4cJLJa","executionInfo":{"status":"ok","timestamp":1638161608612,"user_tz":480,"elapsed":16278,"user":{"displayName":"Arun Talkad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi90KExK9fe7Qcd7jFvDxW6c5tPpI_Xw89GqJnDuw=s64","userId":"10224658465974025190"}},"outputId":"487b6160-4077-471a-9f88-8079d0010271"},"source":["#######################################################################################\n","######################### Testing on testing set ######################################\n","#######################################################################################\n","model.load_state_dict(best_model_wts)\n","model.eval()\n","test_accuracy = test_acc(test_loader)\n","test_l= test_loss(test_loader,0)\n","print(\"===========================\")\n","print(\"Test Acc: {:.7f}, Test Loss: {:.7f} \".format(test_accuracy, test_l))\n","print(opt)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["testing...........\n","===========================\n","Test Acc: 0.5427350, Test Loss: 0.9492438 \n","Namespace(batchSize=100, dataroot='/content/drive/MyDrive/Colab Notebooks/Project/abide_sta/data/ABIDE_pcp/cpac/filt_noglobal', distL='bce', epoch=1, f='/root/.local/share/jupyter/runtime/kernel-e735c474-1b3d-46ca-ab8a-3dadc7f762c4.json', fold=1, gamma=0.5, indim=200, lamb0=1, lamb1=1, lamb2=1, lamb3=0.1, lamb4=0.1, lamb5=0, lamb6=0, layer=2, lr=0.01, matroot='MAT/clear_subjects.mat', n_epochs=100, nclass=2, net='NNGAT', nodes=84, normalization=True, optimizer='Adam', poolmethod='topk', ratio=0.5, rep=30, save_model=True, stepsize=20, weightdecay=0.05)\n"]}]}]}