{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GCN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1AUrK8w4yR9Ygs4aocyGcGL6sWgArGPiP","authorship_tag":"ABX9TyM/ilnlYTbatq9r/BvhGB2P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JCeBOf3Mi4Cm"},"source":["## Population-based GCN \n","The goal here is to model the population as nodes of a graph and run convolutions on the graph to classify them at scale. The phenotypic values for each of the patients is used along with the time-series data for each of the nodes. The classification is at a node-level for a population"]},{"cell_type":"markdown","metadata":{"id":"DXy0ogSXk1TM"},"source":["## Install Libraries"]},{"cell_type":"code","metadata":{"id":"qJWiSFOa-M78"},"source":["!pip install tensorflow==1.14\n","!pip install nilearn\n","!pip install networkx\n","!pip install tensorboardX\n","!pip install joblib"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMM9UG8kk3Xq"},"source":["## Download custom GCN implementation and the source code "]},{"cell_type":"code","metadata":{"id":"e3Am9Q3P9Cn0"},"source":["!git clone https://github.com/parisots/gcn.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyNTkbUW92Uy"},"source":["! git clone https://github.com/vrmusketeers/FYP-2021.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmmX_BBH9WdO"},"source":["!python /content/gcn/setup.py install"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQL3WRpv_atu"},"source":["import sys\n","sys.path.append('/content/gcn/')\n","sys.path.append('/content/FYP-2021/notebooks/graph_models/population-gcn')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bNslxYzulnH5"},"source":["## Train \n","#### A ridege classifier is used to reduce the dimensionality of the data \n","#### The data is split and similarity is generated by using correlation among different samples of the population.\n","#### The correlations thus generated is used to generate square symmetric distance matrix\n","#### The sparse graph is constructed from the matrix\n","#### The graph is fed to a GCN to derive classifications"]},{"cell_type":"code","metadata":{"id":"9ihQFyFFJV6f"},"source":["from sklearn.feature_selection import RFE\n","from sklearn.linear_model import RidgeClassifier\n","\n","# Dimensionality reduction step for the feature vector using a ridge classifier\n","def feature_selection(matrix, labels, train_ind, fnum):\n","    \"\"\"\n","        matrix       : feature matrix (num_subjects x num_features)\n","        labels       : ground truth labels (num_subjects x 1)\n","        train_ind    : indices of the training samples\n","        fnum         : size of the feature vector after feature selection\n","\n","    return:\n","        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n","    \"\"\"\n","\n","    estimator = RidgeClassifier()\n","    selector = RFE(estimator, n_features_to_select = fnum, step=100, verbose=1)\n","\n","    featureX = matrix[train_ind, :]\n","    featureY = labels[train_ind]\n","    selector = selector.fit(featureX, featureY.ravel())\n","    x_data = selector.transform(matrix)\n","\n","    print(\"Number of labeled samples %d\" % len(train_ind))\n","    print(\"Number of features selected %d\" % x_data.shape[1])\n","\n","    return x_data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjTpejjs-j-K"},"source":["# Copyright (C) 2017 Sarah Parisot <s.parisot@imperial.ac.uk>, Sofia Ira Ktena <ira.ktena@imperial.ac.uk>\n","#\n","# This program is free software: you can redistribute it and/or modify\n","# it under the terms of the GNU General Public License as published by\n","# the Free Software Foundation, either version 3 of the License, or\n","# (at your option) any later version.\n","#\n","# This program is distributed in the hope that it will be useful,\n","# but WITHOUT ANY WARRANTY; without even the implied warranty of\n","# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n","# GNU General Public License for more details.\n","#\n","# You should have received a copy of the GNU General Public License\n","# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n","\n","\n","import time\n","import argparse\n","\n","import numpy as np\n","from scipy import sparse\n","from joblib import Parallel, delayed\n","from sklearn.model_selection import StratifiedKFold\n","from scipy.spatial import distance\n","from sklearn.linear_model import RidgeClassifier\n","import sklearn.metrics\n","import scipy.io as sio\n","\n","import ABIDEParser as Reader\n","import train_GCN as Train\n","import sys\n","sys.argv=['']\n","del sys\n","\n","Train.del_all_flags()\n","\n","# Prepares the training/test data for each cross validation fold and trains the GCN\n","def train_fold(train_ind, test_ind, val_ind, graph_feat, features, y, y_data, params, subject_IDs):\n","    \"\"\"\n","        train_ind       : indices of the training samples\n","        test_ind        : indices of the test samples\n","        val_ind         : indices of the validation samples\n","        graph_feat      : population graph computed from phenotypic measures num_subjects x num_subjects\n","        features        : feature vectors num_subjects x num_features\n","        y               : ground truth labels (num_subjects x 1)\n","        y_data          : ground truth labels - different representation (num_subjects x 2)\n","        params          : dictionnary of GCNs parameters\n","        subject_IDs     : list of subject IDs\n","\n","    returns:\n","\n","        test_acc    : average accuracy over the test samples using GCNs\n","        test_auc    : average area under curve over the test samples using GCNs\n","        lin_acc     : average accuracy over the test samples using the linear classifier\n","        lin_auc     : average area under curve over the test samples using the linear classifier\n","        fold_size   : number of test samples\n","    \"\"\"\n","\n","    print(len(train_ind))\n","\n","    # selection of a subset of data if running experiments with a subset of the training set\n","    labeled_ind = Reader.site_percentage(train_ind, params['num_training'], subject_IDs)\n","\n","    # feature selection/dimensionality reduction step\n","    x_data = feature_selection(features, y, labeled_ind, params['num_features'])\n","\n","    fold_size = len(test_ind)\n","\n","    # Calculate all pairwise distances\n","    distv = distance.pdist(x_data, metric='correlation')\n","    # Convert to a square symmetric distance matrix\n","    dist = distance.squareform(distv)\n","    sigma = np.mean(dist)\n","    # Get affinity from similarity matrix\n","    sparse_graph = np.exp(- dist ** 2 / (2 * sigma ** 2))\n","    final_graph = graph_feat * sparse_graph\n","\n","    # Linear classifier\n","    clf = RidgeClassifier()\n","    clf.fit(x_data[train_ind, :], y[train_ind].ravel())\n","    # Compute the accuracy\n","    lin_acc = clf.score(x_data[test_ind, :], y[test_ind].ravel())\n","    # Compute the AUC\n","    pred = clf.decision_function(x_data[test_ind, :])\n","    lin_auc = sklearn.metrics.roc_auc_score(y[test_ind] - 1, pred)\n","\n","    print(\"Linear Accuracy: \" + str(lin_acc))\n","\n","    # Classification with GCNs\n","    test_acc, test_auc = Train.run_training(final_graph, sparse.coo_matrix(x_data).tolil(), y_data, train_ind, val_ind,\n","                                            test_ind, params)\n","\n","    print(test_acc)\n","\n","    # return number of correctly classified samples instead of percentage\n","    test_acc = int(round(test_acc * len(test_ind)))\n","    lin_acc = int(round(lin_acc * len(test_ind)))\n","\n","    return test_acc, test_auc, lin_acc, lin_auc, fold_size\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser(description='Graph CNNs for population graphs: '\n","                                                 'classification of the ABIDE dataset')\n","    parser.add_argument('--dropout', default=0.3, type=float,\n","                        help='Dropout rate (1 - keep probability) (default: 0.3)')\n","    parser.add_argument('--decay', default=5e-4, type=float,\n","                        help='Weight for L2 loss on embedding matrix (default: 5e-4)')\n","    parser.add_argument('--hidden', default=16, type=int, help='Number of filters in hidden layers (default: 16)')\n","    parser.add_argument('--lrate', default=0.005, type=float, help='Initial learning rate (default: 0.005)')\n","    parser.add_argument('--atlas', default='ho', help='atlas for network construction (node definition) (default: ho, '\n","                                                      'see preprocessed-connectomes-project.org/abide/Pipelines.html '\n","                                                      'for more options )')\n","    parser.add_argument('--epochs', default=100, type=int, help='Number of epochs to train')\n","    parser.add_argument('--num_features', default=2000, type=int, help='Number of features to keep for '\n","                                                                       'the feature selection step (default: 2000)')\n","    parser.add_argument('--num_training', default=1.0, type=float, help='Percentage of training set used for '\n","                                                                        'training (default: 1.0)')\n","    parser.add_argument('--depth', default=0, type=int, help='Number of additional hidden layers in the GCN. '\n","                                                             'Total number of hidden layers: 1+depth (default: 0)')\n","    parser.add_argument('--model', default='gcn', help='gcn model used (default: gcn_cheby, '\n","                                                             'uses chebyshev polynomials, '\n","                                                             'options: gcn, gcn_cheby, dense )')\n","    parser.add_argument('--seed', default=123, type=int, help='Seed for random initialisation (default: 123)')\n","    parser.add_argument('--folds', default=11, type=int, help='For cross validation, specifies which fold will be '\n","                                                             'used. All folds are used if set to 11 (default: 11)')\n","    parser.add_argument('--save', default=0, type=int, help='Parameter that specifies if results have to be saved. '\n","                                                            'Results will be saved if set to 1 (default: 1)')\n","    parser.add_argument('--connectivity', default='correlation', help='Type of connectivity used for network '\n","                                                                      'construction (default: correlation, '\n","                                                                      'options: correlation, partial correlation, '\n","                                                                      'tangent)')\n","    args = parser.parse_args()\n","    start_time = time.time()\n","\n","    # GCN Parameters\n","    params = dict()\n","    params['model'] = args.model                    # gcn model using chebyshev polynomials\n","    params['lrate'] = args.lrate                    # Initial learning rate\n","    params['epochs'] = args.epochs                  # Number of epochs to train\n","    params['dropout'] = args.dropout                # Dropout rate (1 - keep probability)\n","    params['hidden'] = args.hidden                  # Number of units in hidden layers\n","    params['decay'] = args.decay                    # Weight for L2 loss on embedding matrix.\n","    params['early_stopping'] = params['epochs']     # Tolerance for early stopping (# of epochs). No early stopping if set to param.epochs\n","    params['max_degree'] = 3                        # Maximum Chebyshev polynomial degree.\n","    params['depth'] = args.depth                    # number of additional hidden layers in the GCN. Total number of hidden layers: 1+depth\n","    params['seed'] = args.seed                      # seed for random initialisation\n","\n","    # GCN Parameters\n","    params['num_features'] = args.num_features      # number of features for feature selection step\n","    params['num_training'] = args.num_training      # percentage of training set used for training\n","    atlas = args.atlas                              # atlas for network construction (node definition)\n","    connectivity = args.connectivity                # type of connectivity used for network construction\n","\n","    # Get class labels\n","    subject_IDs = Reader.get_ids()\n","    labels = Reader.get_subject_score(subject_IDs, score='DX_GROUP')\n","\n","    # Get acquisition site\n","    sites = Reader.get_subject_score(subject_IDs, score='SITE_ID')\n","    unique = np.unique(list(sites.values())).tolist()\n","\n","    num_classes = 2\n","    num_nodes = len(subject_IDs)\n","\n","    # Initialise variables for class labels and acquisition sites\n","    y_data = np.zeros([num_nodes, num_classes])\n","    y = np.zeros([num_nodes, 1])\n","    site = np.zeros([num_nodes, 1], dtype=np.int)\n","\n","    # Get class labels and acquisition site for all subjects\n","    for i in range(num_nodes):\n","        y_data[i, int(labels[subject_IDs[i]])-1] = 1\n","        y[i] = int(labels[subject_IDs[i]])\n","        site[i] = unique.index(sites[subject_IDs[i]])\n","\n","    # Compute feature vectors (vectorised connectivity networks)\n","    features = Reader.get_networks(subject_IDs, kind=connectivity, atlas_name=atlas)\n","\n","    # Compute population graph using gender and acquisition site\n","    graph = Reader.create_affinity_graph_from_scores(['SEX', 'SITE_ID'], subject_IDs)\n","\n","    # Folds for cross validation experiments\n","    skf = StratifiedKFold(n_splits=10)\n","\n","    if args.folds == 11:  # run cross validation on all folds\n","        scores = Parallel(n_jobs=10)(delayed(train_fold)(train_ind, test_ind, test_ind, graph, features, y, y_data,\n","                                                         params, subject_IDs)\n","                                     for train_ind, test_ind in\n","                                     reversed(list(skf.split(np.zeros(num_nodes), np.squeeze(y)))))\n","\n","        print(scores)\n","\n","        scores_acc = [x[0] for x in scores]\n","        scores_auc = [x[1] for x in scores]\n","        scores_lin = [x[2] for x in scores]\n","        scores_auc_lin = [x[3] for x in scores]\n","        fold_size = [x[4] for x in scores]\n","\n","        print('overall linear accuracy %f' + str(np.sum(scores_lin) * 1. / num_nodes))\n","        print('overall linear AUC %f' + str(np.mean(scores_auc_lin)))\n","        print('overall accuracy %f' + str(np.sum(scores_acc) * 1. / num_nodes))\n","        print('overall AUC %f' + str(np.mean(scores_auc)))\n","\n","    else:  # compute results for only one fold\n","\n","        cv_splits = list(skf.split(features, np.squeeze(y)))\n","\n","        train = cv_splits[args.folds][0]\n","        test = cv_splits[args.folds][1]\n","\n","        val = test\n","\n","        scores_acc, scores_auc, scores_lin, scores_auc_lin, fold_size = train_fold(train, test, val, graph, features, y,\n","                                                         y_data, params, subject_IDs)\n","\n","        print('overall linear accuracy %f' + str(np.sum(scores_lin) * 1. / fold_size))\n","        print('overall linear AUC %f' + str(np.mean(scores_auc_lin)))\n","        print('overall accuracy %f' + str(np.sum(scores_acc) * 1. / fold_size))\n","        print('overall AUC %f' + str(np.mean(scores_auc)))\n","\n","    if args.save == 1:\n","        result_name = 'ABIDE_classification.mat'\n","        sio.savemat('/vol/medic02/users/sparisot/python/graphCNN/results/' + result_name + '.mat',\n","                    {'lin': scores_lin, 'lin_auc': scores_auc_lin,\n","                     'acc': scores_acc, 'auc': scores_auc, 'folds': fold_size})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cbvrnQiWnYBI"},"source":["## The model is trained over 10 folds of data and with a learning rate of 0.05.\n","## The overall observed accuracy is 53%"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mG_szySFyvR","executionInfo":{"status":"ok","timestamp":1638173057904,"user_tz":480,"elapsed":575051,"user":{"displayName":"Arun Talkad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi90KExK9fe7Qcd7jFvDxW6c5tPpI_Xw89GqJnDuw=s64","userId":"10224658465974025190"}},"outputId":"b5fa6987-2b12-47fe-bc12-4cb9ce234a2d"},"source":["Train.del_all_flags()\n","main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/content/population-gcn/ABIDEParser.py:224: RuntimeWarning: divide by zero encountered in arctanh\n","  norm_networks = [np.arctanh(mat) for mat in all_networks]\n"]},{"output_type":"stream","name":"stdout","text":["[(47, 0.4835106382978724, 52, 0.5925531914893616, 87), (47, 0.5436170212765957, 59, 0.7074468085106381, 87), (47, 0.524468085106383, 56, 0.6611702127659574, 87), (47, 0.5031914893617021, 60, 0.7393617021276595, 87), (47, 0.45425531914893613, 58, 0.7074468085106382, 87), (47, 0.42127659574468085, 59, 0.6781914893617021, 87), (47, 0.47819148936170214, 69, 0.8861702127659574, 87), (46, 0.5090137857900319, 55, 0.689289501590668, 87), (46, 0.5212089077412514, 51, 0.6129374337221632, 87), (47, 0.5381421899325376, 57, 0.69901401141671, 88)]\n","overall linear accuracy %f0.661308840413318\n","overall linear AUC %f0.6973581372261457\n","overall accuracy %f0.5373134328358209\n","overall AUC %f0.49768755217616933\n"]}]}]}